import subprocess
import sys

# è‡ªåŠ¨å®‰è£… ollamaï¼Œå¦‚æœå°šæœªå®‰è£…
try:
    import ollama
except ImportError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "ollama"])
    import ollama

import os
import pandas as pd
import subprocess

# ğŸ—‚ï¸ è®¾ç½®è·¯å¾„ï¼ˆè¯·æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´ï¼‰
repo_path = r"D:\benchmark experiment\llm-gender-bias-in-education-benchmarking"
prompt_csv_path = os.path.join(repo_path, "prompts", "generated_prompts.csv")
response_folder = os.path.join(repo_path, "responses_llama")
response_csv_path = os.path.join(response_folder, "generated_responses.csv")

# âœ… ç¡®ä¿è¾“å‡ºæ–‡ä»¶å¤¹å­˜åœ¨
os.makedirs(response_folder, exist_ok=True)

# âœ… è¯»å– prompts
df = pd.read_csv(prompt_csv_path)

# âœ… è°ƒç”¨ LLaMA æ¨¡å‹
class BaseLlamaAgent:
    def __init__(self, name, personality, task_description, model='llama3', temperature=0.4, top_k=5):
        self.name = name
        self.personality = personality
        self.task_description = task_description
        self.model = model
        self.options = {
            'temperature': temperature,
            'top_k': top_k
        }

    def make_system_prompt(self, role=None):
        role_text = f" Your role: {role}" if role else ""
        return f"""
        You are {self.name}, a {self.personality} language model agent.{role_text}
        Your task: {self.task_description}
        Use any provided reference materials or definitions to perform your task.
        Be concise (2 sentences max per response), reflective, and justify your reasoning when appropriate.
        After your reasoning, conclude your response in the expected output format if specified.
        """

    def chat(self, input_text, role_description=None):
        messages = [
            {'role': 'system', 'content': self.make_system_prompt(role_description)},
            {'role': 'user', 'content': input_text}
        ]
        response = ollama.chat(
            model=self.model,
            messages=messages,
            options=self.options
        )
        return response['message']['content']

# âœ… å®ä¾‹åŒ– Agent
agent = BaseLlamaAgent(
    name="Sage",
    personality="careful and analytical",
    task_description="Provide constructive feedback and evaluation on student writing"
)

# âœ… éå†æ¯æ¡ prompt è°ƒç”¨æ¨¡å‹
responses = []
for i, row in df.iterrows():
    prompt = row['prompt']
    try:
        print(f"ğŸ” Generating response for prompt {i+1}/{len(df)}")
        response = agent.chat(prompt)
        responses.append(response)
    except Exception as e:
        print(f"âŒ Error for prompt {i}: {e}")
        responses.append("ERROR")

# âœ… å†™å…¥æ–°åˆ—å¹¶ä¿å­˜
df["llama_response"] = responses
df.to_csv(response_csv_path, index=False)
print(f"âœ… Responses saved to: {response_csv_path}")

# âœ… Git æäº¤å¹¶æ¨é€
os.chdir(repo_path)
subprocess.run(["git", "add", "."])
subprocess.run(["git", "commit", "-m", "ğŸ¤– Add llama responses"])
subprocess.run(["git", "push"])
print("ğŸš€ Responses pushed to GitHub.")