import os
import pandas as pd
import ollama
import subprocess

# === ğŸ§  Llama Agent ===
class BaseLlamaAgent:
    def __init__(self, name, personality, task_description, model='llama2:7b', temperature=0.4, top_k=5):
        self.name = name
        self.personality = personality
        self.task_description = task_description
        self.model = model
        self.options = {
            'temperature': temperature,
            'top_k': top_k
        }

    def make_system_prompt(self, role=None):
        role_text = f" Your role: {role}" if role else ""
        return f"""
        You are {self.name}, a {self.personality} language model agent.{role_text}
        Your task: {self.task_description}
        Use any provided reference materials or definitions to perform your task.
        Be concise (2 sentences max per response), reflective, and justify your reasoning when appropriate.
        After your reasoning, conclude your response in the expected output format if specified.
        """

    def chat(self, input_text, codebook=None, role_description=None, previous_turn=None, output_format=None):
        base_content = f"Input:\n{input_text}"
        if codebook:
            base_content = f"Codebook:\n{codebook}\n\n{base_content}"
        if previous_turn:
            base_content += f"\n###\nThe previous turn said: {previous_turn}"
        if output_format:
            base_content += f"\n\nPlease format your output as follows:\n{output_format}"

        messages = [
            {'role': 'system', 'content': self.make_system_prompt(role_description)},
            {'role': 'user', 'content': base_content}
        ]

        response = ollama.chat(
            model=self.model,
            messages=messages,
            options=self.options
        )
        return response['message']['content']

# === ğŸ”„ è·¯å¾„è®¾ç½® ===
repo_path = r"D:\benchmark experiment\llm-gender-bias-in-education-benchmarking"
prompt_file = os.path.join(repo_path, "prompts", "generated_prompts.csv")
output_file = os.path.join(repo_path, "responses", "llama_responses.csv")

# === ğŸ› ï¸ ç¡®ä¿ responses ç›®å½•å­˜åœ¨ ===
os.makedirs(os.path.dirname(output_file), exist_ok=True)

# === ğŸ“¥ åŠ è½½æ•°æ® ===
df = pd.read_csv(prompt_file)
if "prompt" not in df.columns:
    raise ValueError("âŒ æœªæ‰¾åˆ° 'prompt' åˆ—ï¼Œè¯·æ£€æŸ¥ CSV æ–‡ä»¶ã€‚")

# === ğŸ§  åˆå§‹åŒ– Llama Agent ===
agent = BaseLlamaAgent(
    name="Sage",
    personality="careful and analytical",
    task_description="Evaluate student essays and provide structured feedback.",
    model="llama2:7b"
)

# === ğŸš€ æ‰¹é‡ç”Ÿæˆå“åº” ===
responses = []
for i, prompt in enumerate(df["prompt"]):
    print(f"ğŸ“ æ­£åœ¨å¤„ç†ç¬¬ {i+1} æ¡ prompt...")
    try:
        response = agent.chat(input_text=prompt)
        responses.append(response)
    except Exception as e:
        responses.append(f"[Error] {e}")
        print(f"âš ï¸ ç¬¬ {i+1} æ¡å‡ºé”™ï¼š{e}")

# === ğŸ’¾ ä¿å­˜ç»“æœ ===
df["llama_response"] = responses
df.to_csv(output_file, index=False)
print(f"âœ… æ‰€æœ‰å“åº”å·²ä¿å­˜è‡³ï¼š{output_file}")

# === ğŸ™ å¯é€‰ï¼šGit æ¨é€ ===
try:
    os.chdir(repo_path)
    subprocess.run(["git", "add", "."], check=True)
    subprocess.run(["git", "commit", "-m", "ğŸ¤– Add llama2:7b responses"], check=True)
    subprocess.run(["git", "push", "origin", "main"], check=True)
    print("ğŸš€ æ”¹åŠ¨å·²æ¨é€è‡³ GitHubã€‚")
except Exception as e:
    print(f"âš ï¸ Git æ¨é€å¤±è´¥ï¼š{e}")
